üìä MCSA - Rafael Viegas

üìã Pr√©-requisitos
Python 3.8+
pip (gerenciador de pacotes Python)
Git (controle de vers√£o)

üöÄ Instala√ß√£o
1. Clone o reposit√≥rio
git clone https://github.com/seu-usuario/MCSA.git
cd MCSA 

2. Instale as depend√™ncias
pip install -r requirements.txt

üèÉ Execu√ß√£o
Pipeline Completo (ETL + API + Dashboard)
python main.py # Executa todo o processamento de dados

Execu√ß√£o da API FastAPI
# Terminal 1 - Inicie a API REST
python run_api.py

Execu√ß√£o do Dashboard Streamlit
# Terminal 2 - Inicie o dashboard visual
streamlit run viz/app.py

üåê Acessos e Endpoints

üîå API Documentation: http://localhost:8000/docs
üìä Dashboard Interativo: http://localhost:8501
üìö Documenta√ß√£o Alternativa: http://localhost:8000/redoc

Endpoints da API
M√©todo	Endpoint	Descri√ß√£o
GET	/health	Health check da API
GET	/resumo	Resumo com filtros din√¢micos
GET	/resumo/aggregations	Estat√≠sticas agregadas
GET	/resumo/meses	Meses dispon√≠veis
GET	/resumo/credores	Credores dispon√≠veis

üìä Funcionalidades Implementadas

‚úÖ Parte 1: ETL (Extract, Transform, Load)

Extra√ß√£o: Leitura de CSV com tratamento de encoding.
Transforma√ß√£o: Formata√ß√£o de valores.
Limpeza: Tratamento de dados missing e inconsistentes.
Agrupamento: Consolida√ß√£o por CREDOR e STATUS_TITULO.
Carga: Armazenamento em SQLite e CSV.

‚úÖ Parte 2: API FastAPI

Endpoints RESTful: API completa com documenta√ß√£o autom√°tica.
Filtros Din√¢micos: Par√¢metros query para credor, status e m√™s.
Pagina√ß√£o: Controle de limite e offset para grandes datasets.
Valida√ß√£o: Schemas Pydantic para valida√ß√£o de dados.
Tratamento de Erros: Sistema de exce√ß√µes e logging.

‚úÖ Parte 3: Dashboard Streamlit

Visualiza√ß√£o Interativa: Gr√°ficos Plotly com interatividade.
Filtros em Tempo Real: Atualiza√ß√£o din√¢mica dos dados.
M√©tricas em Tempo Real: KPI cards com valores atualizados.
Exporta√ß√£o de Dados: Download dos datasets em CSV.
Design Responsivo: Layout adapt√°vel para diferentes dispositivos.

üß™ Testes e Valida√ß√£o

Testes da API:

# Health check
curl http://localhost:8000/health

# Resumo completo
curl http://localhost:8000/resumo

# Com filtros espec√≠ficos
curl "http://localhost:8000/resumo?credor=Credor+A&status=Pago&mes_ano=2023-01"

Testes do Banco de Dados:

# Verifica√ß√£o da integridade dos dados
python -c "
import sqlite3
conn = sqlite3.connect('data/resumo.bd')
cursor = conn.execute('SELECT COUNT(*) FROM resumo_mensal')
print(f'‚úÖ Registros no banco: {cursor.fetchone()[0]}')
conn.close()
"

Testes de Integra√ß√£o:

# Verifique o pipeline completo
python main.py && echo "‚úÖ ETL executado com sucesso" && python -c "
import requests
response = requests.get('http://localhost:8000/health')
print(f'‚úÖ API Status: {response.json()[\"status\"]}')
"

Uso de IA no Desenvolvimento (Hangzhou DeepSeek):
Declara√ß√£o de Uso de Ferramentas de IA.
Este projeto foi desenvolvido com assist√™ncia estrat√©gica de IA para acelerar o desenvolvimento, garantir boas pr√°ticas de c√≥digo e implementar solu√ß√µes otimizadas.

Como a IA foi utilizada:
1. Gera√ß√£o de Estrutura e Boilerplate:

Contribui√ß√£o: Defini√ß√£o da arquitetura modular e organiza√ß√£o do projeto.

Valida√ß√£o: Estrutura revisada e ajustada para necessidades espec√≠ficas.

2. Implementa√ß√£o do Pipeline ETL:

Modifica√ß√µes: Adapta√ß√£o para o formato espec√≠fico dos dados, adi√ß√£o de valida√ß√µes customizadas e logging detalhado

Valida√ß√£o: Testes com dados reais e verifica√ß√£o de edge cases

3. Leitura, Debug e Documenta√ß√£o de C√≥digo:

Upload dos arquivos de c√≥digo fonte para an√°lise contextual;

Solicita√ß√£o de debugging espec√≠fico para problemas identificados;

An√°lise de performance e sugest√µes de otimiza√ß√£o.

Contribui√ß√£o:

Identifica√ß√£o e corre√ß√£o de bugs complexos de concatena√ß√£o de caminhos.

Melhoria do sistema de tratamento de erros e exce√ß√µes.

Adi√ß√£o de coment√°rios t√©cnicos detalhados para manutenibilidade.

Otimiza√ß√£o de queries SQL e estrutura de dados.

üìä Partes Desenvolvidas Manualmente:
Cria√ß√£oe e parametriza√ß√£o da API FastAPI

Desenvolvimento dos gr√°ficos na interface web.

Integra√ß√£o da Conex√£o ETL ‚Üí API ‚Üí Dashboard.

Sistema de Logging: Implementa√ß√£o de logging detalhado para monitoramento.

Tratamento de Erros: Sistema de exce√ß√µes e fallbacks.

Otimiza√ß√µes de Performance: Melhoria de queries e cache de dados.

Configura√ß√£o de Ambiente: Scripts de setup e documenta√ß√£o.

Valida√ß√µes de Neg√≥cio: Regras espec√≠ficas de dom√≠nio.

‚úÖ M√©todos de Valida√ß√£o Implementados:
Testes Manuais: Todos os endpoints testados via Swagger UI

Valida√ß√£o de Dados: Verifica√ß√£o cruzada entre CSV, SQLite e API responses

Testes de Usabilidade: Avalia√ß√£o da interface e experi√™ncia do usu√°rio

Monitoramento de Performance: An√°lise de tempo de resposta e consumo de mem√≥ria

Valida√ß√£o de Neg√≥cio: Confirma√ß√£o das regras de processamento espec√≠ficas

