ğŸ“Š MCSA - Rafael Viegas

ğŸ—ï¸ Arquitetura do Projeto

MCSA/
â”œâ”€â”€ data/                 # Processamento de dados (ETL)
â”‚   â”œâ”€â”€ dados_cobranca.csv           # Dados originais
â”‚   â”œâ”€â”€ dados_cobranca_formatado.csv # Dados tratados
â”‚   â”œâ”€â”€ resumo_mensal.csv            # Resumo analÃ­tico
â”‚   â”œâ”€â”€ resumo.bd                    # Banco SQLite
â”‚   â”œâ”€â”€ processador_csv.py           # Processamento CSV
â”‚   â””â”€â”€ etl.py                      # Pipeline ETL completo
â”œâ”€â”€ api/                  # API FastAPI
â”‚   â”œâ”€â”€ main.py          # AplicaÃ§Ã£o principal
â”‚   â”œâ”€â”€ models.py        # Modelos Pydantic
â”‚   â””â”€â”€ utils.py         # Utilidades e queries
â”œâ”€â”€ viz/                  # Dashboard Streamlit
â”‚   â””â”€â”€ app.py           # AplicaÃ§Ã£o visual interativa
â”œâ”€â”€ main.py              # Pipeline principal de execuÃ§Ã£o
â”œâ”€â”€ run_api.py           # Inicializador da API
â”œâ”€â”€ requirements.txt     # DependÃªncias do projeto
â””â”€â”€ README.md           # DocumentaÃ§Ã£o completa

ğŸ“‹ PrÃ©-requisitos
Python 3.8+
pip (gerenciador de pacotes Python)
Git (controle de versÃ£o)

ğŸš€ InstalaÃ§Ã£o
1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/MCSA.git
cd MCSA 

2. Instale as dependÃªncias
pip install -r requirements.txt

ğŸƒ ExecuÃ§Ã£o
Pipeline Completo (ETL + API + Dashboard)
python main.py # Executa todo o processamento de dados

ExecuÃ§Ã£o da API FastAPI
# Terminal 1 - Inicie a API REST
python run_api.py

ExecuÃ§Ã£o do Dashboard Streamlit
# Terminal 2 - Inicie o dashboard visual
streamlit run viz/app.py

ğŸŒ Acessos e Endpoints

ğŸ”Œ API Documentation: http://localhost:8000/docs
ğŸ“Š Dashboard Interativo: http://localhost:8501
ğŸ“š DocumentaÃ§Ã£o Alternativa: http://localhost:8000/redoc

Endpoints da API
MÃ©todo	Endpoint	DescriÃ§Ã£o
GET	/health	Health check da API
GET	/resumo	Resumo com filtros dinÃ¢micos
GET	/resumo/aggregations	EstatÃ­sticas agregadas
GET	/resumo/meses	Meses disponÃ­veis
GET	/resumo/credores	Credores disponÃ­veis

ğŸ“Š Funcionalidades Implementadas

âœ… Parte 1: ETL (Extract, Transform, Load)

ExtraÃ§Ã£o: Leitura de CSV com tratamento de encoding.
TransformaÃ§Ã£o: FormataÃ§Ã£o de valores.
Limpeza: Tratamento de dados missing e inconsistentes.
Agrupamento: ConsolidaÃ§Ã£o por CREDOR e STATUS_TITULO.
Carga: Armazenamento em SQLite e CSV.

âœ… Parte 2: API FastAPI

Endpoints RESTful: API completa com documentaÃ§Ã£o automÃ¡tica.
Filtros DinÃ¢micos: ParÃ¢metros query para credor, status e mÃªs.
PaginaÃ§Ã£o: Controle de limite e offset para grandes datasets.
ValidaÃ§Ã£o: Schemas Pydantic para validaÃ§Ã£o de dados.
Tratamento de Erros: Sistema de exceÃ§Ãµes e logging.

âœ… Parte 3: Dashboard Streamlit

VisualizaÃ§Ã£o Interativa: GrÃ¡ficos Plotly com interatividade.
Filtros em Tempo Real: AtualizaÃ§Ã£o dinÃ¢mica dos dados.
MÃ©tricas em Tempo Real: KPI cards com valores atualizados.
ExportaÃ§Ã£o de Dados: Download dos datasets em CSV.
Design Responsivo: Layout adaptÃ¡vel para diferentes dispositivos.

ğŸ§ª Testes e ValidaÃ§Ã£o

Testes da API:

# Health check
curl http://localhost:8000/health

# Resumo completo
curl http://localhost:8000/resumo

# Com filtros especÃ­ficos
curl "http://localhost:8000/resumo?credor=Credor+A&status=Pago&mes_ano=2023-01"

Testes do Banco de Dados:

# VerificaÃ§Ã£o da integridade dos dados
python -c "
import sqlite3
conn = sqlite3.connect('data/resumo.bd')
cursor = conn.execute('SELECT COUNT(*) FROM resumo_mensal')
print(f'âœ… Registros no banco: {cursor.fetchone()[0]}')
conn.close()
"

Testes de IntegraÃ§Ã£o:

# Verifique o pipeline completo
python main.py && echo "âœ… ETL executado com sucesso" && python -c "
import requests
response = requests.get('http://localhost:8000/health')
print(f'âœ… API Status: {response.json()[\"status\"]}')
"

Uso de IA no Desenvolvimento (Hangzhou DeepSeek):
DeclaraÃ§Ã£o de Uso de Ferramentas de IA.
Este projeto foi desenvolvido com assistÃªncia estratÃ©gica de IA para acelerar o desenvolvimento, garantir boas prÃ¡ticas de cÃ³digo e implementar soluÃ§Ãµes otimizadas.

Como a IA foi utilizada:
1. GeraÃ§Ã£o de Estrutura e Boilerplate:

ContribuiÃ§Ã£o: DefiniÃ§Ã£o da arquitetura modular e organizaÃ§Ã£o do projeto.

ValidaÃ§Ã£o: Estrutura revisada e ajustada para necessidades especÃ­ficas.

2. ImplementaÃ§Ã£o do Pipeline ETL:

ModificaÃ§Ãµes: AdaptaÃ§Ã£o para o formato especÃ­fico dos dados, adiÃ§Ã£o de validaÃ§Ãµes customizadas e logging detalhado

ValidaÃ§Ã£o: Testes com dados reais e verificaÃ§Ã£o de edge cases

3. Leitura, Debug e DocumentaÃ§Ã£o de CÃ³digo:

Upload dos arquivos de cÃ³digo fonte para anÃ¡lise contextual;

SolicitaÃ§Ã£o de debugging especÃ­fico para problemas identificados;

AnÃ¡lise de performance e sugestÃµes de otimizaÃ§Ã£o.

ContribuiÃ§Ã£o:

IdentificaÃ§Ã£o e correÃ§Ã£o de bugs complexos de concatenaÃ§Ã£o de caminhos.

Melhoria do sistema de tratamento de erros e exceÃ§Ãµes.

AdiÃ§Ã£o de comentÃ¡rios tÃ©cnicos detalhados para manutenibilidade.

OtimizaÃ§Ã£o de queries SQL e estrutura de dados.

ğŸ“Š Partes Desenvolvidas Manualmente:
CriaÃ§Ã£oe e parametrizaÃ§Ã£o da API FastAPI

Desenvolvimento dos grÃ¡ficos na interface web.

IntegraÃ§Ã£o da ConexÃ£o ETL â†’ API â†’ Dashboard.

Sistema de Logging: ImplementaÃ§Ã£o de logging detalhado para monitoramento.

Tratamento de Erros: Sistema de exceÃ§Ãµes e fallbacks.

OtimizaÃ§Ãµes de Performance: Melhoria de queries e cache de dados.

ConfiguraÃ§Ã£o de Ambiente: Scripts de setup e documentaÃ§Ã£o.

ValidaÃ§Ãµes de NegÃ³cio: Regras especÃ­ficas de domÃ­nio.

âœ… MÃ©todos de ValidaÃ§Ã£o Implementados:
Testes Manuais: Todos os endpoints testados via Swagger UI

ValidaÃ§Ã£o de Dados: VerificaÃ§Ã£o cruzada entre CSV, SQLite e API responses

Testes de Usabilidade: AvaliaÃ§Ã£o da interface e experiÃªncia do usuÃ¡rio

Monitoramento de Performance: AnÃ¡lise de tempo de resposta e consumo de memÃ³ria

ValidaÃ§Ã£o de NegÃ³cio: ConfirmaÃ§Ã£o das regras de processamento especÃ­ficas
